{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "b36399f5",
   "metadata": {},
   "source": [
    "# importing all the libraries used in the project\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from numpy import sqrt \n",
    "import datetime\n",
    "import googlemaps\n",
    "\n",
    "import seaborn as sns\n",
    "import matplotlib as mpl\n",
    "import matplotlib.pyplot as plt\n",
    "import plotly.graph_objects as go\n",
    "\n",
    "from statsmodels.tsa.stattools import adfuller\n",
    "from statsmodels.tsa.arima.model import ARIMA\n",
    "from statsmodels.tsa.statespace.sarimax import SARIMAX\n",
    "from statsmodels.graphics.tsaplots import plot_acf, plot_pacf\n",
    "import statsmodels.graphics.tsaplots as tsa\n",
    "import statsmodels.api as sm\n",
    "from statsmodels.tsa.stattools import kpss\n",
    "from statsmodels.tsa.vector_ar.var_model import VAR\n",
    "\n",
    "import pmdarima as pm\n",
    "from pmdarima import model_selection\n",
    "from pmdarima.utils import decomposed_plot\n",
    "from pmdarima.arima import decompose\n",
    "from pmdarima.arima.stationarity import ADFTest\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.linear_model import LinearRegression\n",
    "from sklearn import metrics\n",
    "from sklearn.metrics import mean_squared_error\n",
    "from sklearn.metrics import r2_score\n",
    "from sklearn.metrics import mean_absolute_error\n",
    "from sklearn.preprocessing import OneHotEncoder\n",
    "from sklearn.compose import make_column_transformer\n",
    "from sklearn.pipeline import make_pipeline\n",
    "from sklearn.model_selection import cross_val_score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6a16132d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# run the dataset through excel\n",
    "bicycledf = pd.read_csv (r'C:\\jp\\first\\data.csv')\n",
    "# did some statistical analysis\n",
    "bicycledf.head()\n",
    "bicycledf.describe()\n",
    "pd.unique(bicycledf.from_station_name)\n",
    "plt.hist(bicycledf.year)\n",
    "bicycledf.info()\n",
    "count = bicycledf.isna().sum()\n",
    "bicycledf.nunique()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e3c7a6b3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# cleaning the data\n",
    "bicycledf['starttime']= pd.to_datetime(bicycledf['starttime'])\n",
    "bicycledf['stoptime']= pd.to_datetime(bicycledf['stoptime'])\n",
    "# make new features\n",
    "bicycledf['WeekDay'] = pd.DatetimeIndex(bicycledf.starttime).weekday\n",
    "bicycledf['WeekDay']=bicycledf['WeekDay'].map({0:'Weekday',1:'Weekday',2:'Weekday',3:'Weekday',4:'Weekday',5:'Weekend',6:'Weekend'})\n",
    "\n",
    "bins = [0,4,8,12,16,20,24]\n",
    "labels = ['Late Night', 'Early Morning','Morning','Noon','Evening','Night']\n",
    "bicycledf['Day_Phase'] = pd.cut(bicycledf['hour'], bins=bins, labels=labels, include_lowest=True)\n",
    "\n",
    "\n",
    "def Seasons(s):\n",
    "    if(s == 1 or s == 2 or s == 12):\n",
    "        return 'Winter'\n",
    "    elif(s == 3 or s == 4 or s == 5):\n",
    "        return 'Spring'\n",
    "    elif(s == 6 or s == 7 or s == 8):\n",
    "        return 'Summer'\n",
    "    elif(s == 9 or s == 10 or s == 11):\n",
    "        return 'Fall'\n",
    "bicycledf['Seasons']=bicycledf.month.apply(Seasons)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d34ca704",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Performing EDA \n",
    "# plotting bar graphs for User Type, Gender and Events\n",
    "fig, (ax1, ax2, ax3) = plt.subplots(nrows=1, ncols=3, figsize=(20,5))\n",
    "\n",
    "ax1.bar(bicycledf.usertype, bicycledf.trip_id)\n",
    "ax1.set_title('Bar graph User Type')\n",
    "ax1.set_xlabel('User type')\n",
    "ax1.yaxis.grid()\n",
    "\n",
    "ax2.bar(bicycledf.gender, bicycledf.trip_id)\n",
    "ax2.set_title('Bar Graph Gender')\n",
    "ax2.set_xlabel('Gender')\n",
    "ax2.yaxis.grid()\n",
    "\n",
    "ax3.bar(bicycledf.events, bicycledf.trip_id)\n",
    "ax3.set_title('Bar Graph Events')\n",
    "ax3.set_xlabel('Events')\n",
    "plt.xticks(rotation=45)\n",
    "ax3.yaxis.grid()\n",
    "\n",
    "# Analysing the data\n",
    "sns.displot(data=bicycledf.tripduration, kde=True)\n",
    "ax[0].set_xlabel('Trip Duration (minutes)', fontsize=12)\n",
    "ax[0].set_title('Trip Duration Distribution')\n",
    "\n",
    "# Number of trips per hour\n",
    "bicycledf_sub = bicycledf.loc[:, ['tripduration', 'starttime']] # Keep only 'starttime' and 'tripduration' variables\n",
    "# Index dataframe by 'datetime64' data in 'starttime' variable\n",
    "bicycledf_sub.index = bicycledf_sub['starttime']\n",
    "weekdays = bicycledf_sub[df_sub.index.weekday < 5]\n",
    "weekends = bicycledf_sub[df_sub.index.weekday > 4]\n",
    "weekdays_countsPerHr = weekdays.groupby(weekdays.index.hour).size()\n",
    "weekends_countsPerHr = weekends.groupby(weekends.index.hour).size()\n",
    "plt.rcParams.update({'font.size': 18, 'legend.fontsize': 20})\n",
    "weekdays_countsPerHr.plot(kind = 'area', stacked = False, figsize = (10, 6), color = 'darkorange',\n",
    "                          linewidth = 2, label='Weekdays')\n",
    "\n",
    "weekends_countsPerHr.plot(kind = 'area', stacked = False, color = 'darkred',\n",
    "                          linewidth = 2, label='Weekends')\n",
    "\n",
    "plt.tick_params(axis = 'both', which = 'major', labelsize = 18)\n",
    "plt.title('Number of trips / hour\\n')\n",
    "plt.xlabel('Time of day (hr)')\n",
    "plt.ylabel('Number of trips')\n",
    "legend = ax.legend(loc='upper left', frameon = False)\n",
    "\n",
    "# Trip distribution\n",
    "fig, ax = plt.subplots(nrows=1, ncols=2, figsize=(16, 6))\n",
    "sns.countplot(x='gender', data=bicycledf, ax=ax[0])\n",
    "ax[0].set_title('Trip Count vs. Gender', fontsize=16)\n",
    "ax[0].set_xlabel('Gender', fontsize=12)\n",
    "ax[0].set_ylabel('Count', fontsize=12)\n",
    "sns.boxplot(x='gender', y='tripduration', data=bicycledf, ax=ax[1])\n",
    "ax[1].set_title('Trip Duration vs. Gender', fontsize=16)\n",
    "ax[1].set_xlabel('Gender', fontsize=12)\n",
    "ax[1].set_ylabel('Trip Duration (minutes)', fontsize=12)\n",
    "plt.show()\n",
    "\n",
    "year_month_pivot = pd.pivot_table(data, index=\"year\", columns=\"month\", values=\"trip_id\", aggfunc=\"count\")\n",
    "fig, ax = plt.subplots(figsize=(10, 6))\n",
    "sns.heatmap(year_month_pivot)\n",
    "plt.title(\"Number of Trips per Year and per Month\")\n",
    "\n",
    "\n",
    "df_group_daytime=bicycledf.groupby(['WeekDay','Day_Phase','gender'],as_index=False).agg({'trip_id': 'count'})\n",
    "males_daytime=df_group_daytime[df_group_daytime.gender == 'Male']\n",
    "females_daytime=df_group_daytime[df_group_daytime.gender == 'Female']\n",
    "r_m_dayphasecount=males_daytime['trip_id'].to_list()\n",
    "r_f_dayphasecount=females_daytime['trip_id'].to_list()\n",
    "df_group_daytime\n",
    "categories = ['Late Night', 'Early Morning','Morning','Noon','Evening','Night']\n",
    "\n",
    "\n",
    "fig = go.Figure()\n",
    "fig.add_trace(go.Scatterpolar(\n",
    "      r=r_m_dayphasecount,\n",
    "      theta=categories,\n",
    "      fill='toself',\n",
    "      name='Male Cyclists'\n",
    "))\n",
    "fig.add_trace(go.Scatterpolar(\n",
    "      r=r_f_dayphasecount,\n",
    "      theta=categories,\n",
    "      fill='toself',\n",
    "      name='Female Cyclists'\n",
    "))\n",
    "\n",
    "fig.update_layout(\n",
    "  polar=dict(\n",
    "    radialaxis=dict(\n",
    "      visible=True,\n",
    "      #range=[0, 2513200]\n",
    "    )),\n",
    "  showlegend=True,\n",
    "    title=\"Cycle Trip count of both males and females during different phases of a day\"\n",
    ")\n",
    "\n",
    "fig.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ce4e42b5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# converting starttime into Date time \n",
    "bicycledf['starttime'] = pd.to_datetime(bicycledf['starttime'], errors='coerce')\n",
    "# Putting starttime as Date time Index\n",
    "bicycledf = bicycledf.set_index(pd.DatetimeIndex(bicycledf['starttime']))\n",
    "# Transforming the data into time series with hourly basis\n",
    "Hourlydata= bicycledf.resample('H').agg({'temperature': np.mean, 'from_station_name': np.count_nonzero, 'gender':np.amax,'Seasons':np.amax,'Day_Phase':np.amax, 'WeekDay':np.amax, 'events':np.amax })\n",
    "Hourlydata= Hourlydata.dropna()\n",
    "# Plotting the time series\n",
    "fig, ax = plt.subplots()\n",
    "ax.set_title('Daily Total Ridership')\n",
    "ax.set_ylabel('Number of Rides')\n",
    "ax.set_xlabel('Date')\n",
    "\n",
    "daily_ridership_overall = Hourlydata['from_station_name']\n",
    "\n",
    "daily_ridership_overall.plot(ax=ax, style = '.');\n",
    "# Plotting the time series but for a few months \n",
    "Hourlydata[(Hourlydata.index > '5/31/2014') & \n",
    "                        (Hourlydata.index < '8/1/2014')]['from_station_name'].plot()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "066ec9c7",
   "metadata": {},
   "outputs": [],
   "source": [
    "ts = Hourlydata['from_station_name']\n",
    "# decomposing the data\n",
    "decomposed_ts = decompose(Hourlydata['from_station_name'].values, 'multiplicative', m=365)\n",
    "\n",
    "decomposed_plot(decomposed_ts, figure_kwargs={'figsize': (16, 10)})\n",
    "plt.show()\n",
    "# ACF plot and PACF plot\n",
    "plot_acf(ts)\n",
    "plt.show()\n",
    "plot_pacf(ts)\n",
    "plt.show()\n",
    "# ADF and KPSS test\n",
    "print(adf_test(ts))\n",
    "def kpss_test(timeseries):\n",
    "    \n",
    "    print ('Results of KPSS Test:')\n",
    "    dftest = kpss(timeseries)\n",
    "    dfoutput = pd.Series(dftest[0:4], index=['Test Statistic','p-value','#Lags Used','Number of Observations Used'])\n",
    "    for key,value in dftest[4].items():\n",
    "       dfoutput['Critical Value (%s)'%key] = value\n",
    "    print (dfoutput)\n",
    "\n",
    "kpss(ts)\n",
    "\n",
    "ts_t_adj = ts - ts.shift(1)\n",
    "ts_t_adj = ts_t_adj.dropna()\n",
    "ts_t_adj.plot()\n",
    "# First Model\n",
    "# Spltting the data\n",
    "test_start = '2017-01-01'\n",
    "Hourlydata['future'] = (Hourlydata.index >= test_start).astype('int')\n",
    "Hourlydata['ride_count_log'] = Hourlydata['from_station_name'].apply(lambda x: np.log(x))\n",
    "Hourlydata['future']\n",
    "\n",
    "train_hourly = Hourlydata[Hourlydata['future'] == 0]['from_station_name']\n",
    "test_hourly = Hourlydata[Hourlydata['future'] == 1]['from_station_name']\n",
    "# Running our First model\n",
    "hourlypred = SARIMAX(train_hourly, order=(2, 0, 1), seasonal_order=(1, 0, 1, 52)).fit(disp=False)\n",
    "dffg = hourlypred.get_forecast(steps = len(test_hourly))\n",
    "y_pred_df = dffg.conf_int(alpha = 0.05) \n",
    "y_pred_df[\"Predictions\"] = hourlypred.predict(start = y_pred_df.index[0], end = y_pred_df.index[-1])\n",
    "test= Hourlydata[Hourlydata.index>= test_start]\n",
    "y_pred_df.index = test.index\n",
    "y_pred_out = y_pred_df[\"Predictions\"]\n",
    "\n",
    "\n",
    "# Checking the Results(RMSE, MSE, R^2 )\n",
    "arma_rmse = np.sqrt(mean_squared_error(test_hourly.values, y_pred_df[\"Predictions\"]))\n",
    "print(\"RMSE: \",arma_rmse)\n",
    "arma_r2 = r2_score(test_hourly.values, y_pred_df[\"Predictions\"]))\n",
    "print(\"R2: \",arma_r2)\n",
    "arma_MSE = mean_squared_error(test_hourly.values, y_pred_df[\"Predictions\"]))\n",
    "print(\"MSE: \",arma_mse)\n",
    "\n",
    "# Preparing our Second model and finding hyper parameters\n",
    "# Taking log\n",
    "Hourlydata['ride_count_log'] = Hourlydata['from_station_name'].apply(lambda x: np.log(x))\n",
    "decomposed_ts1 = decompose(Hourlydata['ride_count_log'].values, 'multiplicative', m=365)\n",
    "decomposed_plot(decomposed_ts1, figure_kwargs={'figsize': (16, 10)})\n",
    "plt.show()\n",
    "ts2 = Hourlydata['ride_count_log']\n",
    "plot_acf(ts2)\n",
    "plt.show()\n",
    "plot_pacf(ts2)\n",
    "plt.show()\n",
    "# Differencing the data\n",
    "ts2diff1 = ts2.diff()\n",
    "ts2diff1 = ts2diff1.dropna()\n",
    "plot_acf(ts2diff1, alpha=0.05)\n",
    "plt.show()\n",
    "plot_pacf(ts2diff1, alpha=0.05)\n",
    "plt.show()\n",
    "ts2diff2 = ts2diff1.diff()\n",
    "ts2diff2 = ts2diff2.dropna()\n",
    "plot_acf(ts2diff2, alpha=0.05)\n",
    "plt.show()\n",
    "plot_pacf(ts2diff2, alpha=0.05)\n",
    "plt.show()\n",
    "ts2diff3 = ts2diff2.diff()\n",
    "ts2diff3 = ts2diff3.dropna()\n",
    "plot_acf(ts2diff3, alpha=0.05)\n",
    "plt.show()\n",
    "plot_pacf(ts2diff3, alpha=0.05)\n",
    "plt.show()\n",
    "# Second Model\n",
    "seas_mod_hourly2 = SARIMAX(train_hourly, order=(1, 3, 1), seasonal_order=(1, 3, 1, 24)).fit(disp=False)\n",
    "dffg2 = seas_mod_hourly2.get_forecast(steps = len(test_hourly))\n",
    "y_pred_df2 = dffg2.conf_int(alpha = 0.05) \n",
    "y_pred_df2[\"Predictions\"] = seas_mod_hourly2.predict(start = y_pred_df2.index[0], end = y_pred_df2.index[-1])\n",
    "test= Hourlydata[Hourlydata.index>= test_start]\n",
    "y_pred_df2.index = test.index\n",
    "y_pred_out2 = y_pred_df2[\"Predictions\"]\n",
    "\n",
    "# Checking the Results(RMSE, MSE, R^2 )\n",
    "arma_rmse = np.sqrt(mean_squared_error(test_hourly.values, y_pred_df2[\"Predictions\"]))\n",
    "print(\"RMSE: \",arma_rmse)\n",
    "arma_r2 = r2_score(test_hourly.values, y_pred_df2[\"Predictions\"]))\n",
    "print(\"R2: \",arma_r2)\n",
    "arma_MSE = mean_squared_error(test_hourly.values, y_pred_df2[\"Predictions\"]))\n",
    "print(\"MSE: \",arma_mse)\n",
    "\n",
    "#Preparing 3rd model\n",
    "# Making Dummy Variables using cat.codes\n",
    "Hourlydata[\"gender\"] = Hourlydata[\"gender\"].astype('category')\n",
    "Hourlydata[\"genderNew\"] = Hourlydata[\"gender\"].cat.codes\n",
    "\n",
    "Hourlydata[\"Seasons\"] = Hourlydata[\"Seasons\"].astype('category')\n",
    "Hourlydata[\"SeasonsNew\"] = Hourlydata[\"Seasons\"].cat.codes\n",
    "\n",
    "Hourlydata[\"Day_Phase\"] = Hourlydata[\"Day_Phase\"].astype('category')\n",
    "Hourlydata[\"Day_PhaseNew\"] = Hourlydata[\"Day_Phase\"].cat.codes\n",
    "\n",
    "Hourlydata[\"WeekDay\"] = Hourlydata[\"WeekDay\"].astype('category')\n",
    "Hourlydata[\"WeekDaynew\"] = Hourlydata[\"WeekDay\"].cat.codes\n",
    "\n",
    "Hourlydata[\"events\"] = Hourlydata[\"events\"].astype('category')\n",
    "Hourlydata[\"eventsnew\"] = Hourlydata[\"events\"].cat.codes\n",
    "\n",
    "Hourlydata.head()\n",
    "# making a linear regression model\n",
    "from patsy import dmatrices\n",
    "test= Hourlydata[Hourlydata.index>= test_start]\n",
    "train= Hourlydata[Hourlydata.index< test_start]\n",
    "expr = 'from_station_name ~ temperature + genderNew + SeasonsNew + Day_PhaseNew + WeekDaynew + eventsnew'\n",
    "y_train, X_train = dmatrices(expr, train, return_type='dataframe')\n",
    "y_test, X_test = dmatrices(expr, test, return_type='dataframe')\n",
    "# OLS fit and summary\n",
    "from statsmodels.regression import linear_model\n",
    "olsr_results = linear_model.OLS(y_train, X_train).fit()\n",
    "olsr_results.summary()\n",
    "# Analysing the new results for hyperparameters\n",
    "tsa.plot_acf(olsr_results.resid, alpha=0.05)\n",
    "plt.show()\n",
    "olsr_results2 = olsr_results.resid.apply(lambda x: np.log(x))\n",
    "olsr_resid_diff_1 = olsr_results2.diff()\n",
    "olsr_resid_diff_1 = olsr_resid_diff_1.dropna()\n",
    "\n",
    "tsa.plot_acf(olsr_resid_diff_1, alpha=0.05)\n",
    "plot_pacf(olsr_resid_diff_1, alpha=0.05)\n",
    "plt.show()\n",
    "\n",
    "dftest = adfuller(olsr_resid_diff_1.dropna().values, autolag = 'AIC')\n",
    "\n",
    "print(\"1. ADF : \",dftest[0])\n",
    "print(\"2. P-Value : \", dftest[1])\n",
    "print(\"3. Num Of Lags : \", dftest[2])\n",
    "print(\"4. Num Of Observations Used For ADF Regression and Critical Values Calculation :\", dftest[3])\n",
    "print(\"5. Critical Values :\")\n",
    "for key, val in dftest[4].items():\n",
    "    print(\"\\t\",key, \": \", val)\n",
    "    \n",
    "olsr_resid_diff_2 = olsr_resid_diff_1.diff()\n",
    "olsr_resid_diff_2 = olsr_resid_diff_2.dropna()\n",
    "plot_acf(olsr_resid_diff_2, alpha=0.05)\n",
    "plot_pacf(olsr_resid_diff_2, alpha=0.05)\n",
    "plt.show()\n",
    "olsr_resid_diff_3 = olsr_resid_diff_2.diff()\n",
    "olsr_resid_diff_3 = olsr_resid_diff_3.dropna()\n",
    "plot_acf(olsr_resid_diff_3, alpha=0.05)\n",
    "plt.show()\n",
    "\n",
    "# Running our 4th model\n",
    "X_train_minus_intercept = X_train.drop('Intercept', axis=1)\n",
    "sarimax_model = ARIMA(endog=y_train, exog=X_train_minus_intercept,order=(2,1,2))\n",
    "sarimax_results = sarimax_model.fit()\n",
    "sarimax_results.summary()\n",
    "\n",
    "#plot results \n",
    "sarimax_results.plot_diagnostics()\n",
    "\n",
    "#forecasting results to compare with test\n",
    "X_test_minus_intercept = X_test.drop('Intercept', axis=1)\n",
    "predictions = sarimax_results.get_forecast(steps=24, exog=X_test_minus_intercept[:24])\n",
    "predictions.summary_frame()\n",
    "\n",
    "#plotting the results\n",
    "predicted, = plt.plot(X_test_minus_intercept[:24].index, predictions.summary_frame()['mean'], 'go-', label='Predicted')\n",
    "actual, = plt.plot(X_test_minus_intercept[:24].index, y_test[:24], 'ro-', label='Actual')\n",
    " lower, = plt.plot(X_test_minus_intercept[:24].index, predictions.summary_frame()['mean_ci_lower'], color='#990099', marker='.', linestyle=':', label='Lower 95%')\n",
    "upper, = plt.plot(X_test_minus_intercept[:24].index, predictions.summary_frame()['mean_ci_upper'], color='#0000cc', marker='.', linestyle=':', label='Upper 95%')\n",
    "plt.fill_between(X_test_minus_intercept[:24].index, predictions.summary_frame()['mean_ci_lower'], predictions.summary_frame()['mean_ci_upper'], color = 'b', alpha = 0.2)\n",
    "plt.legend(handles=[predicted, actual, lower, upper])\n",
    "plt.show()\n",
    "\n",
    "# Making dummy variables using sklearn\n",
    "ohe= OneHotEncoder(sparse= \"FALSE\")\n",
    "Y= Newdf.from_station_id\n",
    "X = Newdf.drop([\"from_station_id\",\"from_station_name\"], axis=\"columns\")\n",
    "X\n",
    "column_Y= make_column_transformer((OneHotEncoder(), [\"gender\", \"Seasons\", \"Day_Phase\", \"WeekDay\", \"events\"]), remainder= \"passthrough\")\n",
    "column_Y.fit_transform(X)\n",
    "# Making Pipeline to connect dummy variable, linear reg and ARIMA model\n",
    "linreg= LinearRegression()\n",
    "# Running our 5th model\n",
    "pipe= make_pipeline(column_Y, linreg, pm.AutoARIMA(seasonal=True, suppress_warnings=True))\n",
    "pipe.fit(train)\n",
    "pipe.predict(5)\n",
    "\n",
    "# Running our 6th model (auto arima ) and comparing models with AIC value\n",
    "\n",
    "model = pm.auto_arima(train[\"from_station_name\"], start_p=1, start_q=1,\n",
    "                      test='adf',       # use adftest to find optimal 'd'\n",
    "                      max_p=3, max_q=3, # maximum p and q\n",
    "                      m=1,              # frequency of series\n",
    "                      d=None,           # let model determine 'd'\n",
    "                      seasonal=False,   # No Seasonality\n",
    "                      start_P=0, \n",
    "                      D=0, \n",
    "                      trace=True,\n",
    "                      error_action='ignore',  \n",
    "                      suppress_warnings=True, \n",
    "                      stepwise=True)\n",
    "# checking model summary\n",
    "print(model.summary())\n",
    "# Running our 7th model (auto arima with seasonality ) and comparing models with AIC value\n",
    "model2 = pm.auto_arima(train[\"from_station_name\"], start_p=1, start_q=1,\n",
    "                      test='adf',       # use adftest to find optimal 'd'\n",
    "                      max_p=3, max_q=3, # maximum p and q\n",
    "                      m=24,              # frequency of series\n",
    "                      d=None,           # let model determine 'd'\n",
    "                      seasonal=True,   # Seasonality\n",
    "                      start_P=0, \n",
    "                      D=0, \n",
    "                      trace=True,\n",
    "                      error_action='ignore',  \n",
    "                      suppress_warnings=True, \n",
    "                      stepwise=True)\n",
    "# checking model summary\n",
    "print(model2.summary())\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0dffa150",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Selecting 10 stations for our new models\n",
    "Nearby = bicycledf.loc(axis=0)[bicycledf['from_station_name'].isin([\"Blackstone Ave & Hyde Park Blvd\",\"Cornell Ave & Hyde Park Blvd\", \"Ellis Ave & 55th St\", \"Ellis Ave & 58th St\", \"Ellis Ave & 60th St\", \"Kimbark Ave & 53rd St\",\"Lake Park Ave & 56th St\", \"Museum of Science and Industry\", \"Shore Dr & 55th St\", \"Woodlawn Ave & 55th St\" ])]\n",
    "Stations =[\"Blackstone Ave & Hyde Park Blvd\",\"Cornell Ave & Hyde Park Blvd\", \"Ellis Ave & 55th St\", \"Ellis Ave & 58th St\", \"Ellis Ave & 60th St\", \"Kimbark Ave & 53rd St\",\"Lake Park Ave & 56th St\", \"Museum of Science and Industry\", \"Shore Dr & 55th St\", \"Woodlawn Ave & 55th St\" ]\n",
    "len(Stations)\n",
    "\n",
    "# Creating a new time series dataset with selected stations and daily basis\n",
    "Newdf=Nearby.groupby([pd.Grouper(freq='1D', key='starttime'),'from_station_name']).agg({'temperature': np.mean, 'from_station_id': np.count_nonzero, 'gender':np.amax,'Seasons':np.amax,'Day_Phase':np.amax, 'WeekDay':np.amax, 'events':np.amax }).sort_index()\n",
    "Newdf = Newdf.reset_index(level=1)\n",
    "Newdf\n",
    "\n",
    "# Creating a pivot table which contains time series of each station\n",
    "df_d=pd.pivot_table(Nearby[['from_station_name','starttime']],aggfunc='count',index=Nearby['starttime'].dt.date,columns=['from_station_name'],fill_value=0)  #date\n",
    "\n",
    "df_d.index = pd.to_datetime(df_d.index)\n",
    "df_d.head()\n",
    "\n",
    "# Create new dictionary to store values\n",
    "df_dict = {}\n",
    "\n",
    "for neighborhood in Stations:\n",
    "    \n",
    "    df_dict[neighborhood] = Newdf\n",
    "\n",
    "# Create new dataframe that contains the sum of ridership of ech station\n",
    "df_neighborhood_sum = pd.DataFrame(index=Stations, columns=['to_station_id'])\n",
    "i = 0\n",
    "for df in df_dict.values():\n",
    "    \n",
    "    df_neighborhood_sum.iloc[i,:] = df_d.iloc[:,i].sum()\n",
    "    i += 1\n",
    "    \n",
    "df_neighborhood_sum.sort_values('to_station_id', inplace=True)\n",
    "\n",
    "# Plotting the sum of ridership\n",
    "fig, ax = plt.subplots(figsize=(10,10))\n",
    "\n",
    "ax.barh(y=df_neighborhood_sum.index, width=df_neighborhood_sum['to_station_id']);\n",
    "\n",
    "# Plotting time series of each station\n",
    "fig, ax = plt.subplots(nrows=len(Stations), figsize=(15,60), sharex=False)\n",
    "i = 0\n",
    "\n",
    "for  neighborhood,index in df_dict.items():\n",
    "    df_d.iloc[:,i].plot(ax=ax[i])\n",
    "    ax[i].set_title(neighborhood)\n",
    "    i += 1\n",
    "\n",
    "    \n",
    "plt.tight_layout();\n",
    "\n",
    "# Performing ADF test on each station\n",
    "df_stationary = pd.DataFrame(index=Stations, columns=['stat_pValue'])\n",
    "\n",
    "for i, index in enumerate(df_dict.values()):\n",
    "    p = adf_test(df_d.iloc[:,i])\n",
    "    \n",
    "    df_stationary.iloc[i,:] = [p]\n",
    "    \n",
    "df_stationary\n",
    "\n",
    "# Create new functions helping in providing results\n",
    "def metricss(y_true, y_pred, print_output):\n",
    "    mae = metrics.mean_absolute_error(y_true, y_pred)\n",
    "    mse = metrics.mean_squared_error(y_true, y_pred, squared=False)\n",
    "    r2 = metrics.r2_score(y_true, y_pred)\n",
    "    \n",
    "    if print_output:\n",
    "       \n",
    "        print(f\"MAE: {mae:,.4f}\")\n",
    "        print(f\"RMSE: {mse:,.4f}\")\n",
    "        print(f\"r^2: {r2:,.4f}\")\n",
    "    \n",
    "    return [mae, mse, r2]\n",
    "\n",
    "# Create another function to run the SARIMA model on each station\n",
    "\n",
    "def run_model(results, df_preds_, neighborhood, ts, order, seasonal_order)\n",
    "    \n",
    "    train = ts[ts['future'] == 0]\n",
    "    test = ts[ts['future'] == 1]\n",
    "\n",
    "    sari_model = SARIMAX(train['ride_count'], order=order, seasonal_order=seasonal_order).fit(maxiter=1000, disp=False)\n",
    "    \n",
    "    # Add preds to predictions DataFrame\n",
    "    preds = sari_model.forecast(steps = len(test))\n",
    "    df_preds_.loc[:,neighborhood] = preds\n",
    "    \n",
    "    # Retrieve metrics\n",
    "    model_results = report_metrics(test['from_station_name'], preds, False)\n",
    "    \n",
    "    # Calculate actual vs. predicted rides for 2021\n",
    "    actual_rides_2018 = ts[ts.index > '12/31/2017']['from_station_name'].sum()\n",
    "    pred_rides_2018 = df_preds_[df_preds_.index > '12/31/2017'][neighborhood].sum()\n",
    "    \n",
    "    ride_delta = np.abs(pred_rides_2021 - actual_rides_2021)\n",
    "    \n",
    "    # Add results to results dataframe\n",
    "    results.loc[neighborhood, 'model'] = sari_model\n",
    "    results.loc[neighborhood, 'order'] = order\n",
    "    results.loc[neighborhood, 'seasonal_order'] = seasonal_order\n",
    "    results.loc[neighborhood, 'MAE'] = model_results[1]\n",
    "    results.loc[neighborhood, 'MSE'] = model_results[2]\n",
    "    results.loc[neighborhood, 'R2'] = model_results[3]\n",
    "    \n",
    "    return results, df_preds_\n",
    "\n",
    "# Running the SARIMA model\n",
    "i=0\n",
    "for neighborhood,index in df_dict.items():\n",
    "    df_results, df_preds = run_model(df_results, df_preds, neighborhood, df_d.iloc[:,i] \n",
    "                                    (1, 1, 0), (0, 1, 1, 52), \n",
    "                                    df_results.loc[neighborhood,'ridership'], i+=1)\n",
    "df_results\n",
    "\n",
    "# Running VAR models\n",
    "# Splitting the data\n",
    "X_trr, X_tes = df_ddf[:int(0.8*(len(df_ddf)))], df_ddf[int(0.8*(len(df_ddf))):]\n",
    "\n",
    "# Differencing\n",
    "X_train_diff =(X_trr).diff().dropna()\n",
    "X_train_diff.describe()\n",
    "\n",
    "# ACF and PACF plots\n",
    "fig, ax = plt.subplots(1,2, figsize=(10,5)) \n",
    "ax[0] = plot_acf(X_train_diff['starttime'][\"Blackstone Ave & Hyde Park Blvd\"], ax=ax[0])\n",
    "ax[1] = plot_pacf(X_train_diff['starttime'][\"Blackstone Ave & Hyde Park Blvd\"], ax=ax[1])\n",
    "\n",
    "# Running the model\n",
    "modelnew = VAR(endog=X_train_log_diff)\n",
    "res = modelnew.select_order(10)\n",
    "res.summary()\n",
    "\n",
    "model_fit = modelnew.fit(maxlags=3)\n",
    "#Print a summary of the model results\n",
    "model_fit.summary()\n",
    "\n",
    "# Get the lag order\n",
    "lag_order = model_fit.k_ar\n",
    "print(lag_order)\n",
    "# Input data for forecasting\n",
    "input_data = X_train_diff.values[-lag_order:]\n",
    "print(input_data)\n",
    "# forecasting\n",
    "pred = model_fit.forecast(y=input_data, steps=290)\n",
    "pred = (pd.DataFrame(pred, index=X_tes.index, columns=[Stations]))\n",
    "print(pred)\n",
    "\n",
    "# Another VAR model\n",
    "# Splitting the data\n",
    "trainstations = df_d[:int(0.8*(len(df_d)))]\n",
    "teststations = df_d[int(0.8*(len(df_d))):]\n",
    "\n",
    "# Run the model\n",
    "model = VAR(endog=trainstations)\n",
    "model_fit = model.fit()\n",
    "prediction = model_fit.forecast(model_fit.y, steps=len(teststations))\n",
    "\n",
    "# Converting test values to data frame\n",
    "testt= pd.DataFrame.from_dict(teststations)\n",
    "testt[\"starttime\"][\"Blackstone Ave & Hyde Park Blvd\"]\n",
    "\n",
    "# Comaring the model with test values\n",
    "pred = pd.DataFrame(index=range(0,len(prediction)),columns=[Stations])\n",
    "for j in range(0,10):\n",
    "    for i in range(0, len(prediction)):\n",
    "       pred.iloc[i][j] = prediction[i][j]\n",
    "\n",
    "#check rmse\n",
    "for i in Stations:\n",
    "    print('rmse value for', i, 'is : ', sqrt(mean_squared_error(pred[i], testt[\"starttime\"][i])))\n",
    "    \n",
    "# Forecasting ridership for each station\n",
    "model = VAR(endog=df_d)\n",
    "model_fit = model.fit()\n",
    "yhat = model_fit.forecast(model_fit.y, steps=20)\n",
    "print(yhat)\n",
    "\n",
    "# Finding Distance matrix for the results\n",
    "\n",
    "gg= Nearby.iloc[[0,1,2,3,14,40002,40016,40028,50009,50027]]\n",
    "# Connecting python to google api\n",
    "gmaps = googlemaps.Client(key = \"API Key\")\n",
    "\n",
    "# no of stations\n",
    "n_Station = 10\n",
    "\n",
    "# creating a distance matrix with zero entries between all pairs of stations\n",
    "stations_distance_matrix = np.zeros((10,10))\n",
    "\n",
    "for i in range(n_Station):\n",
    "    # station, latitude, longitude for origin\n",
    "    origin_city, origin_lat, origin_lng = gg.iloc[i][[\"from_station_name\", \"latitude_start\", \"longitude_start\"]]\n",
    "    \n",
    "    for j in range(i+1, n_Station):\n",
    "        # city, latitude, longitude for destination\n",
    "        dest_city, dest_lat, dest_lng = gg.iloc[j][[\"from_station_name\", \"latitude_start\", \"longitude_start\"]]\n",
    "        # calculate \n",
    "        stations_distance_matrix[i, j] = gmaps.distance_matrix((origin_lat, origin_lng), (dest_lat, dest_lng), \n",
    "                                                             mode=\"bicycling\")[\"rows\"][0][\"elements\"][0][\"distance\"][\"value\"]/1000"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
